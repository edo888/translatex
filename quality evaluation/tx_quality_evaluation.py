# -*- coding: utf-8 -*-
"""TX quality evaluation.ipynb

Automatically generated by Colab.
"""

!pip install pandas pyarrow

!pip install nltk sacrebleu

import pandas as pd

eng_path = 'eng_Latn.parquet'
spa_path = 'spa_Latn.parquet'

COLUMN_NAME = 'text'

# Read and extract first 100 sentences
eng_df = pd.read_parquet(eng_path, engine='pyarrow')
spa_df = pd.read_parquet(spa_path, engine='pyarrow')

eng_sentences = eng_df[COLUMN_NAME].head(100).tolist()
spa_sentences = spa_df[COLUMN_NAME].head(100).tolist()

with open('devtest_english_1-100.txt', 'w', encoding='utf-8') as f:
    for s in eng_sentences:
        f.write(s + '\n')

with open('devtest_spanish_1-100.txt', 'w', encoding='utf-8') as f:
    for s in spa_sentences:
        f.write(s + '\n')

import requests
import urllib.parse

#free
#url = "https://api.translatex.com/translate?sl=en&tl=es&key=AIzaTX..."

#paid
url = "https://api.translatex.com/translate?sl=en&tl=es&key=AIzaTX..."

data = {
    'text': eng_sentences
}

payload_parts = []
for val in data['text']:
    payload_parts.append('text=' + urllib.parse.quote(val))

payload = "&".join(payload_parts)

headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
}

response = requests.post(url, data=payload, headers=headers)

print(response.text)

import json

parsed = json.loads(response.text)

with open('tx_paid_en_es_1-100.txt', 'w', encoding='utf-8') as f:
    for item in parsed['translation']:
        f.write(item + '\n')

import nltk
import sacrebleu

# Make sure you have nltk's punkt tokenizer
nltk.download('punkt')
nltk.download('punkt_tab')

# Read files
with open('google-translate_en_es_1-100.txt', encoding='utf-8') as f:
    hypotheses = [line.strip() for line in f.readlines()]

with open('devtest_spanish_1-100.txt', encoding='utf-8') as f:
    references = [line.strip() for line in f.readlines()]

# Sanity check
assert len(hypotheses) == len(references), "Files must have the same number of lines"

# --- NLTK BLEU (corpus-level) ---
# NLTK expects a list of reference lists and a list of hypothesis sentences (both tokenized)
references_tok = [[nltk.word_tokenize(ref)] for ref in references]  # shape: (N, 1, words)
hypotheses_tok = [nltk.word_tokenize(hyp) for hyp in hypotheses]

# NLTK corpus_bleu expects references as a list of list of reference sentences
corpus_bleu_score = nltk.translate.bleu_score.corpus_bleu(
    references_tok,
    hypotheses_tok
)

print(f"NLTK corpus BLEU score: {corpus_bleu_score * 100:.2f}")

# --- SacreBLEU (corpus-level) ---
# SacreBLEU expects: references=[list of ref sentences] (can be multiple references, hence [ [ref1, ref2, ...] ])
sacrebleu_score = sacrebleu.corpus_bleu(hypotheses, [references])

print(f"SacreBLEU score: {sacrebleu_score.score:.2f}")

!pip install unbabel-comet

import os
os.environ["HF_TOKEN"] = "hf_RV...."

from huggingface_hub import login
login(token=os.environ["HF_TOKEN"])

from comet import download_model, load_from_checkpoint
model_path = download_model("Unbabel/wmt22-cometkiwi-da")
model = load_from_checkpoint(model_path)

with open('google-translate_en_es_1-100.txt', encoding='utf-8') as f:
    sys_outputs = [line.strip() for line in f.readlines()]

with open('devtest_spanish_1-100.txt', encoding='utf-8') as f:
    references = [line.strip() for line in f.readlines()]

assert len(sys_outputs) == len(references), "Mismatch in file lengths!"

with open('devtest_english_1-100.txt', encoding='utf-8') as f:
    sources = [line.strip() for line in f.readlines()]

# Prepare data for COMET
data = [
    {"src": src, "mt": mt, "ref": ref}
    for src, mt, ref in zip(sources, sys_outputs, references)
]

model_output = model.predict(data, batch_size=8)

print(f"COMET score (mean): {model_output['system_score']:.4f}")